# 一元线性回归
一元线性回归：在一个自变量与一个因变量之间建立线性关系。其原理可以简要概括如下：

一、模型假设与表达：一元线性回归模型假设自变量（通常用X表示）和因变量（通常用Y表示）之间存在着线性关系，即Y可以用X的线性函数来表示。也就是说一元线性回归的模型可以表示为 Y = β0 + β1 * X，其中β0和β1是待求参数，β0表示截距，β1表示自变量X的系数。

二、拟合目标与损失函数：我们的目标是找到最合适的参数β0和β1，使得模型对样本数据拟合最好，即使得模型预测值与真实值之间的误差最小化。为了衡量预测值与真实值之间的误差，一元线性回归通常使用最小二乘法作为损失函数，即最小化所有样本数据的均方误差。

三、参数估计：通过最小化损失函数，可以求得最优的β0和β1的值，从而得到最佳拟合的一元线性回归模型。为了合理地估计出β0和β1的值，数学家们引入了最小二乘法和极大似然法。

最小二乘法（Ordinary Least Squares, OLS）的核心是什么呢？使模型预测值与观测值之间的均方误差最小化。从数形结合的角度来看，就是寻找最佳的参数β0和β1，使得拟合的直线能够最好地穿过样本点。

极大似然法（Maximum Likelihood Estimation, MLE）则基于这样一个假设——在模型参数未知的情况下，通过最大化给定观测数据出现的概率来估计这些参数。在一元线性回归中，我们可以假设观测数据（因变量Y）是基于线性模型产生的，而残差则满足正态分布。然后，通过极大似然法，我们可以推导出使得观测数据出现概率最大的β0和β1值，这就得到了一元线性回归模型的参数估计值。

虽然最小二乘法和极大似然法的推导过程不同，但对于满足一定假设条件的数据集，它们得到的参数估计值是相同的。在一元线性回归中，最小二乘法最常用于参数估计，因为它更直观更简便。但是，在某些与统计学相关的应用和推导上，极大似然法使用得也很广泛，尤其在涉及比较复杂的回归模型时，极大似然法能够提供更多的统计性质。

当然，无论使用最小二乘法还是极大似然法，一元线性回归的目标始终是找到最优的拟合直线，以揭示自变量与因变量之间的线性关系。

四、预测：一旦得到最优的β0和β1，就可以用这个模型进行预测，给定一个新的X值，通过模型计算得到对应的Y值。

总结起来，一元线性回归的原理就是通过找到最佳的拟合直线，将自变量X和因变量Y之间的线性关系表示为一个线性函数，从而进行预测和分析。

# 多元线性回归
多元线性回归：在多个自变量与一个因变量之间建立线性关系。其原理可以简要概括如下：

**一、模型假设与表达：** 多元线性回归假设有多个自变量（通常用 X1、X2、...、Xn 表示）与一个因变量（通常用 Y 表示）之间存在线性关系，即 Y 可以用自变量 X1、X2、...、Xn 的线性组合来表示。换句话说，多元线性回归的模型可以表示为 Y = β0 + β1 * X1 + β2 * X2 + ... + βn * Xn，其中 β0 到 βn 是待求的参数，β0 是截距，β1 到 βn 表示自变量 X1 到 Xn 的系数。

**二、拟合目标与损失函数：** 我们的目标是找到最合适的参数 β0 到 βn，使得模型对样本数据拟合最好，即使得模型预测值与真实值之间的误差最小化。为了衡量预测值与真实值之间的误差，多元线性回归同样使用最小二乘法作为损失函数，最小化所有样本数据的均方误差。

**三、参数估计：** 通过最小化损失函数，可以求得最优的 β0 到 βn 的值，从而得到最佳拟合的多元线性回归模型。

**四、预测：** 一旦得到最优的参数 β0 到 βn，就可以用这个模型进行预测。给定新的自变量 X1、X2、...、Xn 值，通过模型计算得到对应的因变量 Y 值。

总结起来，多元线性回归的原理就是通过找到最佳的参数组合，将多个自变量 X1、X2、...、Xn 与一个因变量 Y 之间的线性关系表示为一个线性函数，从而进行预测和分析。多元线性回归在实际应用中广泛用于解决多个特征对一个目标的预测问题，例如房价预测、销售量预测等，使得我们能够更好地理解不同特征与目标之间的关系。

# 对数几率回归

对数几率回归的原理是——通过逻辑函数（Sigmoid函数）将线性组合映射到(0, 1)之间的概率值，并利用最大似然估计找到最优参数来拟合数据和做出分类预测。

具体来说，对数几率回归包括以下几个关键步骤：

1. **逻辑函数：** 对数几率回归使用逻辑函数，也称为Sigmoid函数，将输入的线性组合映射到(0, 1)之间的概率值。逻辑函数的公式为：σ(z) = 1 / (1 + e^(-z))，其中z是输入的线性组合（z = β0 + β1 * X1 + β2 * X2 + ... + βn * Xn）。这个概率值σ(z)表示样本属于正类（类别为1）的概率。

2. **模型表达：** 对数几率回归的模型可以表示为 log(odds) = β0 + β1 * X1 + β2 * X2 + ... + βn * Xn。在这个模型中，β0 到 βn 是待求的参数，表示自变量 X1 到 Xn 的系数。模型的目标是找到最佳的参数组合，使得模型预测的对数几率与实际观测到的对数几率尽可能接近。

3. **最大似然估计：** 对数几率回归使用最大似然估计来估计模型的参数。最大似然估计的目标是找到使观测数据的概率最大化的参数组合。通过最大化样本数据的似然函数（由于概率值在(0, 1)之间，所以用似然函数表示），我们可以找到最优的参数值，使得模型能够最好地拟合已观测到的数据。

4. **预测和分类：** 对数几率回归可以根据逻辑函数计算得到的概率值，来做出分类决策。通常将概率大于等于0.5的样本归为正类（类别为1），概率小于0.5的样本归为负类（类别为0）。这个决策边界就是概率为0.5的那条线或曲面，将两个类别分开。

总结：对数几率回归的核心逻辑是使用逻辑函数将线性组合映射到(0, 1)之间的概率值，并通过最大似然估计找到最优参数，从而使模型能够较好地拟合数据并进行分类预测。它是一种简单而有效的分类算法，在实际应用中得到广泛应用。

# 线性判别分析

线性判别分析（Linear Discriminant Analysis，LDA）的原理是——通过线性变换将高维特征投影到低维空间，并找到最优投影方向，使得不同类别的样本在投影后能够有更好的区分度。

具体来说，线性判别分析的核心逻辑包括以下几个关键步骤：

1. **类别均值计算：** 首先，对于每个类别，计算该类别样本在各个特征上的均值向量。这些均值向量表示每个类别在特征空间中的中心位置。

2. **类内散度矩阵计算：** 计算每个类别的类内散度矩阵。类内散度矩阵衡量了每个类别内部样本的离散程度，用于度量类别内部的散布情况。

3. **类间散度矩阵计算：** 计算类间散度矩阵。类间散度矩阵度量了不同类别之间样本的分离程度，用于度量类别之间的差异性。

4. **投影向量计算：** 通过对类内散度矩阵和类间散度矩阵进行特征值分解，得到最优的投影方向（也称为判别向量）。投影方向是使得样本在低维空间上有更好区分度的方向。

5. **降维：** 选取最优的投影方向，将高维特征样本进行线性变换，将其投影到低维空间。通常情况下，我们选择维度较小的特征子空间来实现降维。

6. **分类决策：** 在低维空间中，使用分类器（例如最近邻算法、支持向量机等）对投影后的样本进行分类决策，从而完成分类任务。

总结：线性判别分析的核心逻辑是通过线性变换将高维特征投影到低维空间，并找到最优的投影方向，以实现样本在投影后的更好区分度。它在处理高维数据时，能够降低计算复杂度，减少冗余信息，同时提高分类性能，因此在实际应用中得到广泛应用。